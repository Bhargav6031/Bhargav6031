{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bhargav6031/advances-in-reinforcement-learning?scriptVersionId=136918899\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"1dff5566","metadata":{"papermill":{"duration":0.005769,"end_time":"2023-07-16T09:04:56.876369","exception":false,"start_time":"2023-07-16T09:04:56.8706","status":"completed"},"tags":[]},"source":["<h2 style=\"text-align: center;\">Reinforcement learning</h2>\n","\n","<p style=\"text-align: center;\"><strong>Author:</strong> Bhargav M gowda </p>\n","<p style=\"text-align: center;\"><strong>Category:</strong>  Other </p>"]},{"cell_type":"markdown","id":"a94b0d55","metadata":{"papermill":{"duration":0.004391,"end_time":"2023-07-16T09:04:56.885705","exception":false,"start_time":"2023-07-16T09:04:56.881314","status":"completed"},"tags":[]},"source":["## Introduction\n","\n","Darwin and spencer wrote 'survival of the fittest' implying those who were most fit would survive the social world due to some biological mechanism that made them superior. In evolution theory, organisms undergo a process of natural selection, where those with advantageous traits or behaviors have a higher likelihood of survival and reproduction. Over generations, these favorable traits become more prevalent in the population. Among them, humans stand tall as the most triumphant species on Mother Earth. What makes humans special is their ability to explore, exploit, generalise and adapt to changing environment or circumstances. If. this process is that good why don't we make use of it?? This led to the birth of a new field of machine learning which will become a pillar of the 21st century's AI revolution."]},{"cell_type":"markdown","id":"a108c1e7","metadata":{"papermill":{"duration":0.004258,"end_time":"2023-07-16T09:04:56.894515","exception":false,"start_time":"2023-07-16T09:04:56.890257","status":"completed"},"tags":[]},"source":["## History of reinforcement learning\n","\n","The history of reinforcement learning (RL) can be traced back to the mid-20th century. The early foundation for RL was laid by the work on Learning Automata (1950s-1960s), explored the idea of using probabilistic models to make adaptive decisions. It was followed by the Dynamic Programming (1957), a theoretical framework for solving optimization problems by breaking them down into smaller subproblems. Q-learning (1980) a model-free RL algorithm that learns the optimal action-value function by iteratively updating Q-values based on observed rewards and state transitions, is one of the foundational algorithms in RL. Temporal Difference Learning (1992) combined ideas from dynamic programming and Monte Carlo methods to enable RL agents to learn from incomplete sequences of experiences and estimate the value functions.  Deep Q-Network (DQN) (2013) proposed by DeepMind, demonstrated the successful integration of deep neural networks with RL. DQN achieved superhuman performance in playing a variety of Atari 2600 games solely based on pixel inputs. AlphaGo (2016) DeepMind's AlphaGo, a RL-based system, made significant headlines when it defeated the world champion Go player, Lee Sedol [1] "]},{"cell_type":"markdown","id":"ee948388","metadata":{"papermill":{"duration":0.004296,"end_time":"2023-07-16T09:04:56.903462","exception":false,"start_time":"2023-07-16T09:04:56.899166","status":"completed"},"tags":[]},"source":["RL algorithms aim to optimize decision-making by iteratively learning from interactions with an environment. Let us understand this with an example of a Dog, ofcourse I'm team dog too :) In the process of training the dogs, we use food as a reward to reinforce desired responses and behaviours from it. When a dog performs a desired action, such as sitting or following a command, they are rewarded with a treat. This reward serves as a positive reinforcement, signaling to the dog that their behavior was correct and increasing the likelihood of them repeating that behavior in the future. \n","\n","[![ezgif-com-crop-2.gif](https://i.postimg.cc/sDrsXzrW/ezgif-com-crop-2.gif)](https://postimg.cc/CdmWPWdM)\n","\n","<p style=\"text-align:center;\">\n","  <strong>Fig.2.</strong> The process of Training Data Development, referenced from [2]\n","</p>\n","In the dog training example, the agent (the dog) interacts with the environment (the training area) based on a specific policy (commands given by the trainer). The dog's actions (sitting, lying down, coming when called) are influenced by its current state (position, trainer's command) and are reinforced through rewards (treats or praise) provided by the trainer. By associating the actions with the rewards, the dog learns to adjust its behavior and improve its performance over time.\n","\n","The concepts of agent, reward, policy, action, state, and environment provide a framework for understanding the dynamics of dog training and how the principles of reinforcement learning can be applied to shape and reinforce desired behaviors in dogs [3]\n","\n","<p style=\"text-align:center;\">\n","  <img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*n1AZU6IkpfjC0l22Md2x0Q.png\" alt=\"drawing\"/>\n","</p>\n","<p style=\"text-align:center;\">\n","  <strong>Fig.2.</strong> Reinforcement learning, referenced from [4]\n","</p>\n","\n","\n"]},{"cell_type":"markdown","id":"7a088610","metadata":{"papermill":{"duration":0.0043,"end_time":"2023-07-16T09:04:56.912253","exception":false,"start_time":"2023-07-16T09:04:56.907953","status":"completed"},"tags":[]},"source":["## A Taxonomy of Reinforcement Learning Algorithms\n","\n","The taxonomy of algorithms in the modern RL space is to highlight the most foundational design choices in deep RL algorithms about what to learn and how to learn it,\n","to expose the trade-offs in those choices, and to place a few prominent modern algorithms into context with respect to those choice.\n","\n","One of the most important branching points in an RL algorithm is the question of whether the agent has access to (or learns) a model of the environment. By a model of the environment, we mean a function which predicts state transitions and rewards. Algorithms which use a model are called model-based methods, and those that don’t are called model-free. While model-free methods forego the potential gains in sample efficiency from using a model, they tend to be easier to implement and tune.[5]\n","\n","<p style=\"text-align:center;\">\n","  <img src=\"https://spinningup.openai.com/en/latest/_images/rl_algorithms_9_15.svg\"/>\n","</p>\n","<p style=\"text-align:center;\">\n","  <strong>Fig.2.</strong> Taxonomy of Reinforcement learning, referenced from [5]\n","</p>"]},{"cell_type":"markdown","id":"ec5c5b74","metadata":{"papermill":{"duration":0.004291,"end_time":"2023-07-16T09:04:56.921052","exception":false,"start_time":"2023-07-16T09:04:56.916761","status":"completed"},"tags":[]},"source":["\n","\n","<h2>Advances in Reinforcement Learning: From Game Playing to Real-World Applications </h2>\n","\n","\n"," Reinforcement learning (RL) has gained immense popularity in the past decade due to its ability to teach machines to learn from experiences and make decisions based on rewards or penalties. Initially, RL was limited to game playing, but in recent years, it has expanded to various real-world applications. We will explore the advances made in RL, from game playing to real-world applications.[6]\n","\n"]},{"cell_type":"markdown","id":"c72fb670","metadata":{"papermill":{"duration":0.004259,"end_time":"2023-07-16T09:04:56.929956","exception":false,"start_time":"2023-07-16T09:04:56.925697","status":"completed"},"tags":[]},"source":["## Reinforcement Learning from Human Feedback : RLHF \n","Language models have shown impressive capabilities in the past few years by generating diverse and compelling text from human input prompts. However, what makes a \"good\" text is inherently hard to define as it is subjective and context dependent. There are many applications such as writing stories where you want creativity, pieces of informative text which should be truthful, or code snippets that we want to be executable.\n","\n","Writing a loss function to capture these attributes seems intractable and most language models are still trained with a simple next token prediction loss (e.g. cross entropy). To compensate for the shortcomings of the loss itself people define metrics that are designed to better capture human preferences such as BLEU or ROUGE. While being better suited than the loss function itself at measuring performance these metrics simply compare generated text to references with simple rules and are thus also limited. Wouldn't it be great if we use human feedback for generated text as a measure of performance or go even one step further and use that feedback as a loss to optimize the model? That's the idea of Reinforcement Learning from Human Feedback (RLHF); use methods from reinforcement learning to directly optimize a language model with human feedback and recursive task decomposition. RLHF has enabled language models to begin to align a model trained on a general corpus of text data to that of complex human values. RLHF's most recent success was its use in ChatGPT.\n","\n","RLHF utilizes small amounts of feedback from a human evaluator to guide the agent’s understanding of the goal and its corresponding reward function. The training process is a three-step feedback cycle. The AI agent starts by randomly acting in the environment. Periodically, the agent presents two video clips of its behavior to the human evaluator, who then decides which clip is closest to fulfilling the goal of a backflip. The agent then uses this feedback to gradually build a model of the goal and the reward function that best explains the human’s judgments. Once the agent has a clear understanding of the goal and the corresponding reward function, it uses RL to learn how to achieve that goal. As its behavior improves, it continues to ask for human feedback on trajectory pairs where it’s most uncertain about which is better, further refining its understanding of the goal.[7]\n","\n","[![5ff1ada8-d384-4c78-9181-022fd5218241-700x395.jpg](https://i.postimg.cc/Gp2cCj1S/5ff1ada8-d384-4c78-9181-022fd5218241-700x395.jpg)](https://postimg.cc/ZWGkPpnr)\n","\n","<p style=\"text-align:center;\">\n","  <strong>Fig.2.</strong> Chatgpt three step cycle, referenced from [8]\n","</p>"]},{"cell_type":"markdown","id":"cd17f179","metadata":{"papermill":{"duration":0.004238,"end_time":"2023-07-16T09:04:56.938661","exception":false,"start_time":"2023-07-16T09:04:56.934423","status":"completed"},"tags":[]},"source":["## Robotics Transformer for real-world control at scale: RT1\n","\n","Major recent advances in multiple subfields of machine learning (ML) research, such as computer vision and natural language processing, have been enabled by a shared common approach that leverages large, diverse datasets and expressive models that can absorb all of the data effectively. Although there have been various attempts to apply this approach to robotics, robots have not yet leveraged highly-capable models as well as other subfields.\n","\n","Several factors contribute to this challenge. First, there’s the lack of large-scale and diverse robotic data, which limits a model’s ability to absorb a broad set of robotic experiences. Data collection is particularly expensive and challenging for robotics because dataset curation requires engineering-heavy autonomous operation, or demonstrations collected using human teleoperations. A second factor is the lack of expressive, scalable, and fast-enough-for-real-time-inference models that can learn from such datasets and generalize effectively.\n","\n","To address these challenges, Google propose the Robotics Transformer 1 (RT-1), a multi-task model that tokenizes robot inputs and outputs actions (e.g., camera images, task instructions, and motor commands) to enable efficient inference at runtime, which makes real-time control feasible. This model is trained on a large-scale, real-world robotics dataset of 130k episodes that cover 700+ tasks, collected using a fleet of 13 robots from Everyday Robots (EDR) over 17 months. Google demonstrates that RT-1 can exhibit significantly improved zero-shot generalization to new tasks, environments and objects compared to prior techniques, and Google has open-sourced the RT-1 code, with a hope that will provide a valuable resource for future research on scaling up robot learning. [9]"]},{"cell_type":"markdown","id":"e6b0b519","metadata":{"papermill":{"duration":0.004243,"end_time":"2023-07-16T09:04:56.947372","exception":false,"start_time":"2023-07-16T09:04:56.943129","status":"completed"},"tags":[]},"source":["## Pre-Training for Robots\n","Progress in deep learning highlights the tremendous potential of utilizing diverse robotic datasets for attaining effective generalization and makes it enticing to consider leveraging broad datasets for attaining robust generalization in robotic learning as well. However, in practice, we often want to learn a new skill in a new environment that is unlikely to be contained in the prior data. Therefore we ask: how can we leverage existing diverse offline datasets in combination with small amounts of task-specific data to solve new tasks, while still enjoying the generalization benefits of training on large amounts of data?  Researchers from UC Berkely demonstrate that end-to-end offline RL can be an effective approach for doing this, without the need for any representation learning or vision-based pre-training. \n","    \n","   Pre-training for robots (PTR), a framework based on offline RL that attempts to effectively learn new tasks by combining pre-training on existing robotic datasets with rapid fine-tuning on a new task, with as few as 10 demonstrations. PTR utilizes an existing offline RL method, conservative Q-learning (CQL), but extends it to include several crucial design decisions that enable PTR to actually work and outperform a variety of prior methods. PTR is the one of the first RL method that succeeds at learning new tasks in a new domain on a real WidowX robot with as few as 10 task demonstrations, by effectively leveraging an existing dataset of diverse multi-task robot data collected in a variety of toy kitchens. They also demonstrate that PTR can enable effective autonomous fine-tuning and improvement in a handful of trials, without needing any demonstrations.[10]"]},{"cell_type":"markdown","id":"f4b1b401","metadata":{"papermill":{"duration":0.004266,"end_time":"2023-07-16T09:04:56.957655","exception":false,"start_time":"2023-07-16T09:04:56.953389","status":"completed"},"tags":[]},"source":["## Interfacing language models with Robots : SayCan \n","\n","Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could in principle be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack contextual grounding, which makes it difficult to leverage them for decision making within a given real-world context. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. SayCan is proposed to provide this grounding by means of pretrained behaviors, which are used to condition the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model’s “hands and eyes,” while the language model supplies high-level semantic knowledge about the task. \n","\n","Imagine a robot operating in a kitchen that is capable of executing skills such as \"pick up the coffee cup\" or \"go to the sink\". To get the robot to use these skills to perform a complex task (e.g. \"I spilled my drink, can you help?\"), the user could manually break it up into steps consisting of these atomic commands. However,this would be exceedingly tedious. A language model can split the high-level instruction (\"I spilled my drink, can you help?\") into sub-tasks, but it cannot do that effectively unless it has the context of what the robot is capable of given the abilities, current state of the robot and its environment[11]\n","\n","[![palm-saycan-teaser-compressed.gif](https://i.postimg.cc/26WNkc9T/palm-saycan-teaser-compressed.gif)](https://postimg.cc/LYmbvTpZ)\n","\n","<p style=\"text-align:center;\">\n","  <strong>Fig.2.</strong> SayCan, referenced from [12]\n","</p>\n"]},{"cell_type":"markdown","id":"62cb366f","metadata":{"papermill":{"duration":0.004258,"end_time":"2023-07-16T09:04:56.96641","exception":false,"start_time":"2023-07-16T09:04:56.962152","status":"completed"},"tags":[]},"source":["## Magnetic Control of nuclear fusion Plasmas in Tokamak \n","\n","\n","A tokamak is the most advanced of current fusion machine designs in order to get fusion reactions going. Although there are many designs, the Tokamak a Soviet design is the best way we have found so far. Tokamak stands for toroidal chamber magnetic coils, it's a ring doughnut shaped vessel with superconducting magnetic coils around it and uses a blend of deuterium and tritium as fuel mix. The fuel should be kept hot enough, dense enough and long enough like the center of a star for fusion reactions to occur. Inside the tokamak the fuel is heated to produce a plasma, a superheated gas in the fourth state of matter. . The hot plasma(150 million degree celcius) is kept away from the walls of the tokamak by a magnetic field produced primarily by two sets of coils that contain the plasma horizontally and vertically. Fast control systems can change the magnetic field produced by these magnets to keep the plasma trapped and stable. The conventional approaches is to  precompute a set of feedforward coil currents and voltages, Then designing single-input single-output PID controllers to stabilize the plasma vertical position and control the radial position and plasma current. But, the controllers are designed on the basis of linearized model dynamics, and gain scheduling is required to track time-varying control targets.\n","\n","\n","A radically new approach to controller design is made possible by using reinforcement learning (RL) to generate non-linear feedback controllers."]},{"cell_type":"markdown","id":"eef351c7","metadata":{"papermill":{"duration":0.004216,"end_time":"2023-07-16T09:04:56.975113","exception":false,"start_time":"2023-07-16T09:04:56.970897","status":"completed"},"tags":[]},"source":["## Game Playing\n","\n","Game playing is the most commonly used application of RL. Games such as chess, Go, and poker have been used as benchmarks for RL algorithms. A significant milestone occurred in 1997 when IBM’s “Deep Blue” AI system achieved a notable breakthrough by defeating world chess champion Garry Kasparov. Despite demonstrating the capabilities of AI in gaming, Deep Blue’s effectiveness was dependent on human programming and restricted to the game of chess. \n","\n","OpenAI Five is the first AI to beat the world champions in an esports game, having won two back-to-back games versus the world champion Dota 2 team. OpenAI Five plays 180 years worth of games against itself every day, learning via self-play. It trains using a scaled-up version of Proximal Policy Optimization running on 256 GPUs and 128,000 CPU cores. Despite decades of work, the strongest Go computer programs could only play at the level of human amateurs.\n","\n","\n","Go originated in China over 3,000 years ago. Winning this board game requires multiple layers of strategic thinking. There are an astonishing 10 to the power of 170 possible board configurations - more than the number of atoms in the known universe. Standard AI methods, which test all possible moves and positions using a search tree, can’t handle the sheer number of possible Go moves or evaluate the strength of each possible board position. In the last decade, a remarkable revolution has taken place in AI-based GO game development. The driving force behind this revolution is AlphaZero. Unlike Deep Blue, that heavily rely on human expertise and pre-programmed heuristics, AlphaZero has the ability to learn from scratch through self-play without relying on human-generated data or pre-programmed strategies. It continuously plays against itself to improve its skills and strategies over time. \n","\n","Scientists at DeepMind wanted to evaluate it's performance. Hence they organised a faceoff between world's professional Go player of 9 dan rank Lee Sedol and AlphaGo, AlphaGo won all but the fourth game. He was one of the strongest players in the history of Go.The match has been compared with the historic chess match between Deep Blue and Garry Kasparov in 1997. Deep Blue's Murray Campbell called AlphaGo's victory \"the end of an era... board games are more or less done and it's time to move on.\" \n","\n","[![Alpha-Go-vs-Lee-Sodol-Alpha-Go-Movie.jpg](https://i.postimg.cc/C5VdybfZ/Alpha-Go-vs-Lee-Sodol-Alpha-Go-Movie.jpg)](https://postimg.cc/FdDh3dpm)\n","\n","<p>\"I thought AlphaGo was based on probability calculation and that it was merely a machine. But when I saw its moves, I changed my mind. Surely, AlphaGo is creative.\" - <strong>Lee Sedol</strong><p>\n","\n","Two years later, its successor - AlphaZero - learned from scratch to master Go, chess and shogi with a single algorithm. Expanding upon this foundation, **MuZero**, an advanced iteration of AlphaZero, has extended and enhanced these abilities even further. The ability to plan is an important part of human intelligence, allowing us to solve problems and make decisions about the future. For example, if we see dark clouds forming we might predict it will rain and decide to take an umbrella with us before we go out. Researchers have tried to tackle this major challenge in AI by using two main approaches: lookahead search or model-based planning. Systems that use lookahead search, such as AlphaZero, have achieved remarkable success in classic games such as checkers, chess and poker, but rely on being given knowledge of their environment’s dynamics. Model-based systems aim to address this issue by learning an accurate model of an environment’s dynamics, and then using it to plan. However, the complexity of modelling every aspect of an environment has meant these algorithms are unable to compete in visually rich domains, such as Atari. MuZero uses a different approach to overcome the limitations of previous approaches. Instead of trying to model the entire environment, MuZero just models aspects that are important to the agent’s decision-making process. After all, knowing an umbrella will keep you dry is more useful to know than modelling the pattern of raindrops in the air.\n","\n","    \n","<h3>BBF Agent: Learns 26 Games in 2 Hours</h3>\n","DeepMind, in collaboration with researchers from Mila and Université de Montréal, has introduced an AI agent that defies these limitations. This agent known as the Bigger, Better, Faster (BBF) model, has achieved superhuman performance on Atari benchmarks while learning 26 games in just two hours. The success of BBF can be attributed to its unique model-free learning approach. By relying on rewards and punishments received through interactions with the game world, BBF bypasses the need to construct an explicit game model. This streamlined process lets the agent focus solely on learning and optimizing its performance, resulting in faster and more efficient training.[13]\n","\n","\n","<p></p>\n","The use of RL in game playing has several advantages. Firstly, games have well-defined rules and objectives, making it easier to define the reward function for the agent. Secondly, the outcome of a game is deterministic, which means that the agent can learn from its mistakes and improve its performance. Finally, game playing is a safe environment to test and improve RL algorithms without the risk of physical harm. "]},{"cell_type":"markdown","id":"f5288fbe","metadata":{"papermill":{"duration":0.004248,"end_time":"2023-07-16T09:04:56.983951","exception":false,"start_time":"2023-07-16T09:04:56.979703","status":"completed"},"tags":[]},"source":["## Challenges\n","\n","Despite the promising applications of RL in various domains, there are still several challenges that need to be addressed. One of the main challenges is the sample inefficiency of RL algorithms. In the field of NLP RL alarlhflgosgos has the challenge to optimise human sentiments over human preferences. RL algorithms require a large number of interactions with the environment to learn optimal policies, which can be time-consuming and expensive. Another challenge is the difficulty of defining the reward function. In some domains, such as healthcare, the reward function may be unclear or difficult to define, which can hinder the performance of the RL algorithm.\n","\n","Another challenge is the safety and ethical concerns associated with RL applications. RL algorithms can learn to optimize for the reward function without considering the long-term consequences or ethical implications of their actions. This can lead to unintended consequences or unethical behavior, such as biased decision-making or harm to humans or the environment. Finally, there is a lack of interpretability and transparency in RL algorithms. RL algorithms can be complex and difficult to understand, making it challenging to explain their decisions or identify potential biases or errors.\n","\n","## Conclusion\n","\n","Reinforcement learning has come a long way from its early applications in game playing to its current use in various real-world domains. RL has the potential to revolutionize healthcare, finance, robotics, and other domains by providing personalized solutions, improving efficiency, and reducing costs. However, there are still several challenges that need to be addressed, including sample inefficiency, reward function definition, safety and ethical concerns, and interpretability. Researchers and practitioners must work together to address these challenges and ensure that RL applications are safe, effective, and beneficial for society. "]},{"cell_type":"markdown","id":"4aff08d8","metadata":{"papermill":{"duration":0.004229,"end_time":"2023-07-16T09:04:56.992644","exception":false,"start_time":"2023-07-16T09:04:56.988415","status":"completed"},"tags":[]},"source":["## References\n","\n","\n","\n","[1] History of Reinforcement Learning http://incompleteideas.net/book/ebook/node12.html\n","\n","[2] Dribble https://dribbble.com/shots/10736771-Dog-Training-Animations\n","\n","[3] Positive reinforcement training https://www.humanesociety.org/resources/positive-reinforcement-training\n","\n","[4]  3.1 The Agent-Environment Interface http://www.incompleteideas.net/book/ebook/node28.html\n","\n","[5] Open AI Spinning Up https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html\n","\n","[6] Advances in Reinforcement Learning https://www.linkedin.com/company/codersarts/\n","\n","[7] RLHF : https://huggingface.co/blog/rlhf\n","\n","[8] Three step cycle chatgpt https://openai.com/blog/chatgpt\n","\n","[9] RT1 : https://arxiv.org/abs/2212.06817\n","\n","[10] Pretrained Robots https://arxiv.org/abs/2210.05178\n","\n","[11] arXiv: Do As I Can, Not As I Say: Grounding Language in Robotic Affordances https://arxiv.org/abs/2204.01691\n","\n","[12] SayCan https://say-can.github.io/\n","\n","[13] Bigger, Better, Faster: Human-level Atari with human-level efficiency https://arxiv.org/abs/2305.19452"]},{"cell_type":"code","execution_count":1,"id":"c24e2008","metadata":{"execution":{"iopub.execute_input":"2023-07-16T09:04:57.004147Z","iopub.status.busy":"2023-07-16T09:04:57.003413Z","iopub.status.idle":"2023-07-16T09:04:57.057945Z","shell.execute_reply":"2023-07-16T09:04:57.057032Z"},"papermill":{"duration":0.062967,"end_time":"2023-07-16T09:04:57.060105","exception":false,"start_time":"2023-07-16T09:04:56.997138","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>type</th>\n","      <th>value</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>essay_category</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>essay_url</td>\n","      <td>https://www.kaggle.com/code/bhargav6031/notebo...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>feedback1_url</td>\n","      <td>http://www.kaggle.com/.../your_1st_peer_feedback</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>feedback2_url</td>\n","      <td>http://www.kaggle.com/.../your_2nd_peer_feedback</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>feedback3_url</td>\n","      <td>http://www.kaggle.com/.../your_3rd_peer_feedback</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             type                                              value\n","0  essay_category                                              Other\n","1       essay_url  https://www.kaggle.com/code/bhargav6031/notebo...\n","2   feedback1_url   http://www.kaggle.com/.../your_1st_peer_feedback\n","3   feedback2_url   http://www.kaggle.com/.../your_2nd_peer_feedback\n","4   feedback3_url   http://www.kaggle.com/.../your_3rd_peer_feedback"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","df = pd.read_csv(\"/kaggle/input/2023-kaggle-ai-report/sample_submission.csv\")\n","df.loc[df['type'] == 'essay_category', 'value'] = 'Other'\n","df.loc[df['type'] == 'essay_url', 'value'] =  'https://www.kaggle.com/code/bhargav6031/notebookc7ce652e23'\n","df.loc[df['type'] == 'feedback1_url', 'value'] = 'http://www.kaggle.com/.../your_1st_peer_feedback'\n","df.loc[df['type'] == 'feedback2_url', 'value'] = 'http://www.kaggle.com/.../your_2nd_peer_feedback'\n","df.loc[df['type'] == 'feedback3_url', 'value'] = 'http://www.kaggle.com/.../your_3rd_peer_feedback'\n","df.head()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"papermill":{"default_parameters":{},"duration":14.743474,"end_time":"2023-07-16T09:04:57.88728","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-07-16T09:04:43.143806","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}