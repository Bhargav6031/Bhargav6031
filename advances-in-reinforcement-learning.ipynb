{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bhargav6031/advances-in-reinforcement-learning?scriptVersionId=136862106\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<h2 style=\"text-align: center;\">Reinforcement learning</h2>\n\n<p style=\"text-align: center;\"><strong>Author:</strong> Bhargav M gowda </p>\n<p style=\"text-align: center;\"><strong>Category:</strong>  Other </p>","metadata":{"papermill":{"duration":0.00313,"end_time":"2023-07-05T19:56:26.045945","exception":false,"start_time":"2023-07-05T19:56:26.042815","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Introduction\n\nDarwin and spencer wrote 'survival of the fittest' implying those who were most fit would survive the social world due to some biological mechanism that made them superior. In evolution theory, organisms undergo a process of natural selection, where those with advantageous traits or behaviors have a higher likelihood of survival and reproduction. Over generations, these favorable traits become more prevalent in the population. Among them, humans stand tall as the most triumphant species on Mother Earth. What makes humans special is their ability to explore, exploit, generalise and adapt to changing environment or circumstances. If we this process is that good why don't we make use of it?? This led to the birth of a new field of machine learning which will become a pillar of the 21st century's AI revolution.","metadata":{"papermill":{"duration":0.002132,"end_time":"2023-07-05T19:56:26.050652","exception":false,"start_time":"2023-07-05T19:56:26.04852","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## History of reinforcement learning\n\nThe history of reinforcement learning (RL) can be traced back to the mid-20th century. The early foundation for RL was laid by the work on Learning Automata (1950s-1960s), explored the idea of using probabilistic models to make adaptive decisions. It was followed by the Dynamic Programming (1957), a theoretical framework for solving optimization problems by breaking them down into smaller subproblems. Q-learning (1980) a model-free RL algorithm that learns the optimal action-value function by iteratively updating Q-values based on observed rewards and state transitions, is one of the foundational algorithms in RL. Temporal Difference Learning (1992) combined ideas from dynamic programming and Monte Carlo methods to enable RL agents to learn from incomplete sequences of experiences and estimate the value functions.  Deep Q-Network (DQN) (2013) proposed by DeepMind, demonstrated the successful integration of deep neural networks with RL. DQN achieved superhuman performance in playing a variety of Atari 2600 games solely based on pixel inputs. AlphaGo (2016) DeepMind's AlphaGo, a RL-based system, made significant headlines when it defeated the world champion Go player, Lee Sedol [1] ","metadata":{}},{"cell_type":"markdown","source":"RL algorithms aim to optimize decision-making by iteratively learning from interactions with an environment. Let us understand this with an example of a Dog, ofcourse I'm team dog too :). In the process of training the dogs, we use food as a reward to reinforce desired responses and behaviours from it. When a dog performs a desired action, such as sitting or following a command, they are rewarded with a treat. This reward serves as a positive reinforcement, signaling to the dog that their behavior was correct and increasing the likelihood of them repeating that behavior in the future. \n\n[![ezgif-com-crop-2.gif](https://i.postimg.cc/sDrsXzrW/ezgif-com-crop-2.gif)](https://postimg.cc/CdmWPWdM)\n\n<p style=\"text-align:center;\">\n  <strong>Fig.2.</strong> The process of Training Data Development, referenced from [2]\n</p>\nIn the dog training example, the agent (the dog) interacts with the environment (the training area) based on a specific policy (commands given by the trainer). The dog's actions (sitting, lying down, coming when called) are influenced by its current state (position, trainer's command) and are reinforced through rewards (treats or praise) provided by the trainer. By associating the actions with the rewards, the dog learns to adjust its behavior and improve its performance over time.\n\nThe concepts of agent, reward, policy, action, state, and environment provide a framework for understanding the dynamics of dog training and how the principles of reinforcement learning can be applied to shape and reinforce desired behaviors in dogs [3]\n\n<p style=\"text-align:center;\">\n  <img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*n1AZU6IkpfjC0l22Md2x0Q.png\" alt=\"drawing\"/>\n</p>\n<p style=\"text-align:center;\">\n  <strong>Fig.2.</strong> Reinforcement learning, referenced from [4]\n</p>\n","metadata":{}},{"cell_type":"markdown","source":"## A Taxonomy of Reinforcement Learning Algorithms\n\nThe taxonomy of algorithms in the modern RL space is to highlight the most foundational design choices in deep RL algorithms about what to learn and how to learn it,\nto expose the trade-offs in those choices, and to place a few prominent modern algorithms into context with respect to those choice.\n\nOne of the most important branching points in an RL algorithm is the question of whether the agent has access to (or learns) a model of the environment. By a model of the environment, we mean a function which predicts state transitions and rewards. Algorithms which use a model are called model-based methods, and those that don’t are called model-free. While model-free methods forego the potential gains in sample efficiency from using a model, they tend to be easier to implement and tune.[5]\n\n<p style=\"text-align:center;\">\n  <img src=\"https://spinningup.openai.com/en/latest/_images/rl_algorithms_9_15.svg\"/>\n</p>\n<p style=\"text-align:center;\">\n  <strong>Fig.2.</strong> Taxonomy of Reinforcement learning, referenced from [5]\n</p>","metadata":{}},{"cell_type":"markdown","source":"\n\n<h2>Advances in Reinforcement Learning: From Game Playing to Real-World Applications </h2>\n\n\n Reinforcement learning (RL) has gained immense popularity in the past decade due to its ability to teach machines to learn from experiences and make decisions based on rewards or penalties. Initially, RL was limited to game playing, but in recent years, it has expanded to various real-world applications. We will explore the advances made in RL, from game playing to real-world applications.[6]\n\n","metadata":{"papermill":{"duration":0.001928,"end_time":"2023-07-05T19:56:26.055097","exception":false,"start_time":"2023-07-05T19:56:26.053169","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Reinforcement Learning from Human Feedback : RLHF \nLanguage models have shown impressive capabilities in the past few years by generating diverse and compelling text from human input prompts. However, what makes a \"good\" text is inherently hard to define as it is subjective and context dependent. There are many applications such as writing stories where you want creativity, pieces of informative text which should be truthful, or code snippets that we want to be executable.\n\nWriting a loss function to capture these attributes seems intractable and most language models are still trained with a simple next token prediction loss (e.g. cross entropy). To compensate for the shortcomings of the loss itself people define metrics that are designed to better capture human preferences such as BLEU or ROUGE. While being better suited than the loss function itself at measuring performance these metrics simply compare generated text to references with simple rules and are thus also limited. Wouldn't it be great if we use human feedback for generated text as a measure of performance or go even one step further and use that feedback as a loss to optimize the model? That's the idea of Reinforcement Learning from Human Feedback (RLHF); use methods from reinforcement learning to directly optimize a language model with human feedback and recursive task decomposition. RLHF has enabled language models to begin to align a model trained on a general corpus of text data to that of complex human values. RLHF's most recent success was its use in ChatGPT.\n\nRLHF utilizes small amounts of feedback from a human evaluator to guide the agent’s understanding of the goal and its corresponding reward function. The training process is a three-step feedback cycle. The AI agent starts by randomly acting in the environment. Periodically, the agent presents two video clips of its behavior to the human evaluator, who then decides which clip is closest to fulfilling the goal of a backflip. The agent then uses this feedback to gradually build a model of the goal and the reward function that best explains the human’s judgments. Once the agent has a clear understanding of the goal and the corresponding reward function, it uses RL to learn how to achieve that goal. As its behavior improves, it continues to ask for human feedback on trajectory pairs where it’s most uncertain about which is better, further refining its understanding of the goal.[7]\n\n[![5ff1ada8-d384-4c78-9181-022fd5218241-700x395.jpg](https://i.postimg.cc/Gp2cCj1S/5ff1ada8-d384-4c78-9181-022fd5218241-700x395.jpg)](https://postimg.cc/ZWGkPpnr)\n\n<p style=\"text-align:center;\">\n  <strong>Fig.2.</strong> Chatgpt three step cycle, referenced from [8]\n</p>","metadata":{}},{"cell_type":"markdown","source":"## Robotics Transformer for real-world control at scale: RT1\n\nMajor recent advances in multiple subfields of machine learning (ML) research, such as computer vision and natural language processing, have been enabled by a shared common approach that leverages large, diverse datasets and expressive models that can absorb all of the data effectively. Although there have been various attempts to apply this approach to robotics, robots have not yet leveraged highly-capable models as well as other subfields.\n\nSeveral factors contribute to this challenge. First, there’s the lack of large-scale and diverse robotic data, which limits a model’s ability to absorb a broad set of robotic experiences. Data collection is particularly expensive and challenging for robotics because dataset curation requires engineering-heavy autonomous operation, or demonstrations collected using human teleoperations. A second factor is the lack of expressive, scalable, and fast-enough-for-real-time-inference models that can learn from such datasets and generalize effectively.\n\nTo address these challenges, Google propose the Robotics Transformer 1 (RT-1), a multi-task model that tokenizes robot inputs and outputs actions (e.g., camera images, task instructions, and motor commands) to enable efficient inference at runtime, which makes real-time control feasible. This model is trained on a large-scale, real-world robotics dataset of 130k episodes that cover 700+ tasks, collected using a fleet of 13 robots from Everyday Robots (EDR) over 17 months. Google demonstrates that RT-1 can exhibit significantly improved zero-shot generalization to new tasks, environments and objects compared to prior techniques, and Google has open-sourced the RT-1 code, with a hope that will provide a valuable resource for future research on scaling up robot learning. [9]","metadata":{}},{"cell_type":"markdown","source":"## Pre-Training for Robots\nProgress in deep learning highlights the tremendous potential of utilizing diverse robotic datasets for attaining effective generalization and makes it enticing to consider leveraging broad datasets for attaining robust generalization in robotic learning as well. However, in practice, we often want to learn a new skill in a new environment that is unlikely to be contained in the prior data. Therefore we ask: how can we leverage existing diverse offline datasets in combination with small amounts of task-specific data to solve new tasks, while still enjoying the generalization benefits of training on large amounts of data?  Researchers from UC Berkely demonstrate that end-to-end offline RL can be an effective approach for doing this, without the need for any representation learning or vision-based pre-training. \n    \n   Pre-training for robots (PTR), a framework based on offline RL that attempts to effectively learn new tasks by combining pre-training on existing robotic datasets with rapid fine-tuning on a new task, with as few as 10 demonstrations. PTR utilizes an existing offline RL method, conservative Q-learning (CQL), but extends it to include several crucial design decisions that enable PTR to actually work and outperform a variety of prior methods. PTR is the one of the first RL method that succeeds at learning new tasks in a new domain on a real WidowX robot with as few as 10 task demonstrations, by effectively leveraging an existing dataset of diverse multi-task robot data collected in a variety of toy kitchens. They also demonstrate that PTR can enable effective autonomous fine-tuning and improvement in a handful of trials, without needing any demonstrations.[10]","metadata":{}},{"cell_type":"markdown","source":"## Interfacing language models with Robots : SayCan \n\nLarge language models can encode a wealth of semantic knowledge about the world. Such knowledge could in principle be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack contextual grounding, which makes it difficult to leverage them for decision making within a given real-world context. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. SayCan is proposed to provide this grounding by means of pretrained behaviors, which are used to condition the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model’s “hands and eyes,” while the language model supplies high-level semantic knowledge about the task. \n\nImagine a robot operating in a kitchen that is capable of executing skills such as \"pick up the coffee cup\" or \"go to the sink\". To get the robot to use these skills to perform a complex task (e.g. \"I spilled my drink, can you help?\"), the user could manually break it up into steps consisting of these atomic commands. However,this would be exceedingly tedious. A language model can split the high-level instruction (\"I spilled my drink, can you help?\") into sub-tasks, but it cannot do that effectively unless it has the context of what the robot is capable of given the abilities, current state of the robot and its environment[11]\n\n[![palm-saycan-teaser-compressed.gif](https://i.postimg.cc/26WNkc9T/palm-saycan-teaser-compressed.gif)](https://postimg.cc/LYmbvTpZ)\n\n<p style=\"text-align:center;\">\n  <strong>Fig.2.</strong> SayCan, referenced from [12]\n</p>\n","metadata":{}},{"cell_type":"markdown","source":"## Game Playing\n\nGame playing is the most commonly used application of RL. Games such as chess, Go, and poker have been used as benchmarks for RL algorithms. A significant milestone occurred in 1997 when IBM’s “Deep Blue” AI system achieved a notable breakthrough by defeating world chess champion Garry Kasparov. Despite demonstrating the capabilities of AI in gaming, Deep Blue’s effectiveness was dependent on human programming and restricted to the game of chess. Despite decades of work, the strongest Go computer programs could only play at the level of human amateurs. Standard AI methods, which test all possible moves and positions using a search tree, can’t handle the sheer number of possible Go moves or evaluate the strength of each possible board position.\n\n\nGo originated in China over 3,000 years ago. Winning this board game requires multiple layers of strategic thinking. There are an astonishing 10 to the power of 170 possible board configurations - more than the number of atoms in the known universe. In the last decade, a remarkable revolution has taken place in AI-based GO game development. The driving force behind this revolution is AlphaZero. Unlike Deep Blue, that heavily rely on human expertise and pre-programmed heuristics, AlphaZero has the ability to learn from scratch through self-play without relying on human-generated data or pre-programmed strategies. It continuously plays against itself to improve its skills and strategies over time. \n\nScientists at AlphaZero wanted to evaluate it's performance. Hence they organised a faceoff between world's professional Go player of 9 dan rank Lee Sedol and AlphaGo, AlphaGo won all but the fourth game. He was one of the strongest players in the history of Go.The match has been compared with the historic chess match between Deep Blue and Garry Kasparov in 1997. Deep Blue's Murray Campbell called AlphaGo's victory \"the end of an era... board games are more or less done and it's time to move on.\" This was the statement of Lee Sedol, Winner of 18 world Go titles after losing the torunament.\n\n[![Alpha-Go-vs-Lee-Sodol-Alpha-Go-Movie.jpg](https://i.postimg.cc/C5VdybfZ/Alpha-Go-vs-Lee-Sodol-Alpha-Go-Movie.jpg)](https://postimg.cc/FdDh3dpm)[13]\n\n    \"I thought AlphaGo was based on probability calculation and that it was merely a machine. But when I saw its moves, I changed my mind. Surely, AlphaGo is creative.\"\n                        \n                        \n\n\n\nThe use of RL in game playing has several advantages. Firstly, games have well-defined rules and objectives, making it easier to define the reward function for the agent. Secondly, the outcome of a game is deterministic, which means that the agent can learn from its mistakes and improve its performance. Finally, game playing is a safe environment to test and improve RL algorithms without the risk of physical harm. ","metadata":{}},{"cell_type":"markdown","source":"## Challenges\n\nDespite the promising applications of RL in various domains, there are still several challenges that need to be addressed. One of the main challenges is the sample inefficiency of RL algorithms. In the field of NLP RL arlhflgos has the challenge to optimise human sentiments over human preferences. RL algorithms require a large number of interactions with the environment to learn optimal policies, which can be time-consuming and expensive. Another challenge is the difficulty of defining the reward function. In some domains, such as healthcare, the reward function may be unclear or difficult to define, which can hinder the performance of the RL algorithm.\n\nAnother challenge is the safety and ethical concerns associated with RL applications. RL algorithms can learn to optimize for the reward function without considering the long-term consequences or ethical implications of their actions. This can lead to unintended consequences or unethical behavior, such as biased decision-making or harm to humans or the environment. Finally, there is a lack of interpretability and transparency in RL algorithms. RL algorithms can be complex and difficult to understand, making it challenging to explain their decisions or identify potential biases or errors.\n\n## Conclusion\n\nReinforcement learning has come a long way from its early applications in game playing to its current use in various real-world domains. RL has the potential to revolutionize healthcare, finance, robotics, and other domains by providing personalized solutions, improving efficiency, and reducing costs. However, there are still several challenges that need to be addressed, including sample inefficiency, reward function definition, safety and ethical concerns, and interpretability. Researchers and practitioners must work together to address these challenges and ensure that RL applications are safe, effective, and beneficial for society. ","metadata":{}},{"cell_type":"markdown","source":"## References\n\n\n\n[1] History of Reinforcement Learning http://incompleteideas.net/book/ebook/node12.html\n\n[2] Dribble https://dribbble.com/shots/10736771-Dog-Training-Animations\n\n[3] Positive reinforcement training https://www.humanesociety.org/resources/positive-reinforcement-training\n\n[4]  3.1 The Agent-Environment Interface http://www.incompleteideas.net/book/ebook/node28.html\n\n[5] Open AI Spinning Up https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html\n\n[6] Advances in Reinforcement Learning https://www.linkedin.com/company/codersarts/\n\n[7] RLHF : https://huggingface.co/blog/rlhf\n\n[8] Three step cycle chatgpt https://openai.com/blog/chatgpt\n\n[9] RT1 : https://arxiv.org/abs/2212.06817\n\n[10] Pretrained Robots https://arxiv.org/abs/2210.05178\n\n[11] arXiv: Do As I Can, Not As I Say: Grounding Language in Robotic Affordances https://arxiv.org/abs/2204.01691\n\n[12] SayCan https://say-can.github.io/\n\n","metadata":{"papermill":{"duration":0.002489,"end_time":"2023-07-05T19:56:26.405323","exception":false,"start_time":"2023-07-05T19:56:26.402834","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"/kaggle/input/2023-kaggle-ai-report/sample_submission.csv\")\ndf.loc[df['type'] == 'essay_category', 'value'] = 'Other'\ndf.loc[df['type'] == 'essay_url', 'value'] =  'https://www.kaggle.com/code/bhargav6031/notebookc7ce652e23'\ndf.loc[df['type'] == 'feedback1_url', 'value'] = 'http://www.kaggle.com/.../your_1st_peer_feedback'\ndf.loc[df['type'] == 'feedback2_url', 'value'] = 'http://www.kaggle.com/.../your_2nd_peer_feedback'\ndf.loc[df['type'] == 'feedback3_url', 'value'] = 'http://www.kaggle.com/.../your_3rd_peer_feedback'\ndf.head()","metadata":{"papermill":{"duration":0.002375,"end_time":"2023-07-05T19:56:26.410394","exception":false,"start_time":"2023-07-05T19:56:26.408019","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}